<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>FlashAttention 和 PagedAttention</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../llm/flash_attention_and_paged_attention.html">LLM Boosting</a></li><li class="breadcrumb-item"><a href="../llm/flash_attention_and_paged_attention.html">FlashAttention 和 PagedAttention</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">KPatr1ck’s Blog</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/KPatr1ck/KPatr1ck.github.io">
            Source Code
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">LLM Boosting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../llm/flash_attention_and_paged_attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">FlashAttention 和 PagedAttention</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#参考文献" id="toc-参考文献" class="nav-link active" data-scroll-target="#参考文献">0. 参考文献</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">1. Overview</a></li>
  <li><a href="#onlinesoftmax" id="toc-onlinesoftmax" class="nav-link" data-scroll-target="#onlinesoftmax">2. OnlineSoftmax</a>
  <ul class="collapse">
  <li><a href="#softmax" id="toc-softmax" class="nav-link" data-scroll-target="#softmax">2.1 Softmax</a></li>
  <li><a href="#safesoftmax" id="toc-safesoftmax" class="nav-link" data-scroll-target="#safesoftmax">2.2 SafeSoftmax</a></li>
  <li><a href="#onlinesoftmax-1" id="toc-onlinesoftmax-1" class="nav-link" data-scroll-target="#onlinesoftmax-1">2.3 OnlineSoftmax</a></li>
  <li><a href="#代码实现" id="toc-代码实现" class="nav-link" data-scroll-target="#代码实现">2.4 代码实现</a></li>
  </ul></li>
  <li><a href="#flashattention" id="toc-flashattention" class="nav-link" data-scroll-target="#flashattention">3. FlashAttention</a>
  <ul class="collapse">
  <li><a href="#compute-bound-vs-memory-bound" id="toc-compute-bound-vs-memory-bound" class="nav-link" data-scroll-target="#compute-bound-vs-memory-bound">3.1 Compute Bound Vs Memory Bound</a></li>
  <li><a href="#gpu-memory-layout" id="toc-gpu-memory-layout" class="nav-link" data-scroll-target="#gpu-memory-layout">3.2 GPU Memory Layout</a></li>
  <li><a href="#standard-attention" id="toc-standard-attention" class="nav-link" data-scroll-target="#standard-attention">3.3 Standard Attention</a></li>
  <li><a href="#flashattention-1" id="toc-flashattention-1" class="nav-link" data-scroll-target="#flashattention-1">3.4 FlashAttention</a></li>
  <li><a href="#代码实现-1" id="toc-代码实现-1" class="nav-link" data-scroll-target="#代码实现-1">3.5 代码实现</a></li>
  </ul></li>
  <li><a href="#pagedattention" id="toc-pagedattention" class="nav-link" data-scroll-target="#pagedattention">4. PagedAttention</a>
  <ul class="collapse">
  <li><a href="#kv-cache" id="toc-kv-cache" class="nav-link" data-scroll-target="#kv-cache">4.1 KV Cache</a></li>
  <li><a href="#memory-fragmentation" id="toc-memory-fragmentation" class="nav-link" data-scroll-target="#memory-fragmentation">4.2 Memory Fragmentation</a></li>
  <li><a href="#pagedattention-1" id="toc-pagedattention-1" class="nav-link" data-scroll-target="#pagedattention-1">4.3 PagedAttention</a></li>
  </ul></li>
  <li><a href="#抛砖引玉" id="toc-抛砖引玉" class="nav-link" data-scroll-target="#抛砖引玉">5. 抛砖引玉</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">FlashAttention 和 PagedAttention</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="参考文献" class="level2">
<h2 class="anchored" data-anchor-id="参考文献">0. 参考文献</h2>
<ul>
<li><a href="https://arxiv.org/abs/1805.02867">[8 May 2018]Online normalizer calculation for softmax</a></li>
<li><a href="https://arxiv.org/abs/2205.14135">[27 May 2022]FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
<li><a href="https://arxiv.org/abs/2307.08691">[17 Jul 2023]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
<li><a href="https://arxiv.org/abs/2309.06180">[12 Sep 2023]Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/638468472">NLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a></li>
<li><a href="https://blog.vllm.ai/2024/06/20/vllm.html">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</a></li>
<li><a href="https://www.zhihu.com/question/611236756/answer/3132304304">FlashAttention 的速度优化原理是怎样的？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/638788074">一心二用的Online Softmax</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/669926191">图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎</a></li>
</ul>
<hr>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">1. Overview</h2>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Term</th>
<th style="text-align: left;">是什么</th>
<th style="text-align: left;">用在哪里</th>
<th style="text-align: left;">技术亮点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Softmax</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">SafeSoftmax</td>
<td style="text-align: left;">为了防止 Softmax 计算过程中 exp 函数出现数值上下溢的情况，提出的一种数值处理方案</td>
<td style="text-align: left;">Torch,TF,Paddle 等框架，嵌入至绝大多数模型</td>
<td style="text-align: left;">数值处理，指数部分减去原始向量的 max 值。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OnlineSoftmax</td>
<td style="text-align: left;">Softmax 的一种在线更新的计算方法</td>
<td style="text-align: left;">任何需要计算 Softmax 的场景</td>
<td style="text-align: left;">降低对原始向量的遍历次数，计算速度加快。</td>
</tr>
<tr class="even">
<td style="text-align: left;">FlashAttention</td>
<td style="text-align: left;">一种 <strong>精确的</strong>（非近似） Attention 的加速算法</td>
<td style="text-align: left;">所有用到 Attention OP 的模型，包括不限于类 Bert，类 GPT 等模型。</td>
<td style="text-align: left;">结合 GPU Memory 的硬件 Layout，基于 Online Softmax 对 Attention OP 进行 <strong>精细化</strong> 优化。不降 FLOPs，降低 HBM 读写次数和算子融合达到加速效果。<br>时间复杂度 <strong>不变</strong>，空间复杂度从 <span class="math inline">\(O(N^2)\)</span> 降低至 <span class="math inline">\(O(N)\)</span>。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FlashAttention-2</td>
<td style="text-align: left;">基于 FlashAttention 算法，提升 FlashAttention 的计算效率</td>
<td style="text-align: left;">与 FlashAttention 一致</td>
<td style="text-align: left;">FlashAttention 计算提效，提升了非 matmul 的 FLOPs，引入了更好的并行化计算方式和内存复用。</td>
</tr>
<tr class="even">
<td style="text-align: left;">PagedAttention</td>
<td style="text-align: left;">一种提升模型服务 <strong>吞吐量</strong> 的算法，提升批量计算时显存的利用效率</td>
<td style="text-align: left;">vLLM 加速框架的核心技术，用于各种用到用到 Decoder 模型的服务中，适用于市面上各类 LLM</td>
<td style="text-align: left;">LLM 推理框架（如 FT 和 Orca）在批量推理时，由于用于存储 KV Cache 的显存需要提前分配，并针对每一个序列分配在连续的显存空间上，导致存在大量的碎片（存在于 <strong>序列内</strong> 和 <strong>序列间</strong>）。<br>PagedAttention 引入内存分页管理的思想，用一个 Scheduler 单例，以 Block 为单位动态地分配显存，提升显存利用率（Batch 计算中复用，Decode 过程中剪枝等），从而加速批量计算，提升模型服务的吞吐量。</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="onlinesoftmax" class="level2">
<h2 class="anchored" data-anchor-id="onlinesoftmax">2. OnlineSoftmax</h2>
<section id="softmax" class="level3">
<h3 class="anchored" data-anchor-id="softmax">2.1 Softmax</h3>
<p><span class="math display">\[ y = softmax(x) \]</span> <span class="math display">\[ y_i = \frac{e^{x_i}}{\sum_{j=1}^{V} e^{x_j}}  \tag{1}\]</span></p>
<p>where <span class="math inline">\(x, y \in \mathbb{R}^V\)</span> . The naive implementation (see algorithm <code>1</code>) scans the input vector two times - one to calculate the normalization term <span class="math inline">\(d_V\)</span> and another to compute output values <span class="math inline">\(y_i\)</span> - effectively doing <strong>three memory accesses</strong> per vector element: two loads and one store.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/d20eb1b8.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">d20eb1b8.png</figcaption>
</figure>
</div>
</section>
<section id="safesoftmax" class="level3">
<h3 class="anchored" data-anchor-id="safesoftmax">2.2 SafeSoftmax</h3>
<p>Unfortunately, on real hardware, where the range of numbers represented is limited, the line <code>3</code> of the algorithm <code>1</code> can <strong>overflow or underflow due to the exponent</strong>. There is a safe form of <code>(1)</code>, which is immune to this problem:</p>
<p><span class="math display">\[ y_i = \frac{e^{x_i-\max_{k=1}^{V}x_k}}{\sum_{j=1}^{V} e^{x_j-\max_{k=1}^{V}x_k}}  \tag{2}\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/de4cc3d9.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">de4cc3d9.png</figcaption>
</figure>
</div>
<p>But Safe Softmax does three passes over input vector: The first one calculates the maximum value <span class="math inline">\(m_V\)</span> , the second one - normalization term <span class="math inline">\(d_V\)</span> , and the third one - final values <span class="math inline">\(y_i\)</span>, see algorithm <code>2</code>; This results in <strong>4 memory access</strong> per vector element overall.</p>
</section>
<section id="onlinesoftmax-1" class="level3">
<h3 class="anchored" data-anchor-id="onlinesoftmax-1">2.3 OnlineSoftmax</h3>
<p>The algorithm <code>3</code> calculates both the maximum value m and the normalization term <span class="math inline">\(d\)</span> in a <strong>single pass</strong> over input vector with negligible additional cost of two operations per vector element. It <strong>reduces memory accesses</strong> from 4 down to 3 per vector element for the Softmax function evaluation. Inspiration came from the numerically stable variance calculation online algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/ba540d94.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">ba540d94.png</figcaption>
</figure>
</div>
<p>Essentially, the algorithm keeps the maximum value <span class="math inline">\(m\)</span> and the normalization term <span class="math inline">\(d\)</span> as it iterates over elements of the input array. At each iteration it <strong>needs to adjust</strong> the normalizer <span class="math inline">\(d\)</span> to the new maximum <span class="math inline">\(m_j\)</span> and only then add new value to the normalizer.</p>
<p>这里需要注意的点在于，<span class="math inline">\(m_j\)</span> 和 <span class="math inline">\(d_j\)</span> 是在同一遍 Loop 中实时更新的，<span class="math inline">\(m_j\)</span> 求 max，很容易更新；<span class="math inline">\(d_j\)</span> 是指数项求和，在 Loop 中一旦发生最大值更新，即 <span class="math inline">\(m_j \ne m_{j-1}\)</span> ，<span class="math inline">\(d_{j-1}\)</span> 则需要进行一个指数项的缩放，其<strong>数值</strong>为 <span class="math inline">\(e^{m_{j-1}-m_j}\)</span>。</p>
<p>实际上这里还可以继续优化，把 <span class="math inline">\(y\)</span> 的计算也放在第一遍 Loop 中，如果这么做，每一次 iter 需要更新的值是 <span class="math inline">\(\{y_1, y_2...y_{j-1}\}\)</span>，然后拼接上最新的 <span class="math inline">\({y_j}\)</span>，缩放的方式需要同时考虑分子的 <span class="math inline">\(m\)</span> 和分母中的 <span class="math inline">\(v\)</span>，其<strong>数值</strong>为 <span class="math inline">\(\frac{d_{j-1} * e^{m_{j-1}-m_j}}{d_j}\)</span>。(注意针对 <span class="math inline">\(y\)</span> 的缩放和针对 <span class="math inline">\(d\)</span> 的缩放的差异)</p>
</section>
<section id="代码实现" class="level3">
<h3 class="anchored" data-anchor-id="代码实现">2.4 代码实现</h3>
<p>OnlineSoftmax实现，如 <code>2.3</code> 最后的结论，用单 Loop 完成 SafeSoftmax 的数值计算，且支持 chunk 内并行。</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> online_softmax(x: np.ndarray, chunk_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    num_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(x.shape[<span class="dv">1</span>] <span class="op">/</span> chunk_size))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>]))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>]))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    softmax <span class="op">=</span> np.zeros_like(x)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get chunk data</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        chunk_x <span class="op">=</span> x[:, i <span class="op">*</span> chunk_size : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> chunk_size]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 1. Perform local softmax on chunk data.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        chunk_m <span class="op">=</span> np.<span class="bu">max</span>(chunk_x, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        chunk_f <span class="op">=</span> np.exp(chunk_x <span class="op">-</span> np.expand_dims(chunk_m, <span class="dv">1</span>))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        chunk_d <span class="op">=</span> np.<span class="bu">sum</span>(chunk_f, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        chunk_softmax <span class="op">=</span> chunk_f <span class="op">/</span> np.expand_dims(chunk_d, <span class="dv">1</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 2. Get new m_x and l_x.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        m_new <span class="op">=</span> np.maximum(m, chunk_m)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        d_new <span class="op">=</span> d <span class="op">*</span> np.exp(m <span class="op">-</span> m_new) <span class="op">+</span> chunk_d <span class="op">*</span> np.exp(chunk_m <span class="op">-</span> m_new)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 3. Rescale(OLD + NEW) and concat(NEW) softmax values.</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rescale</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        softmax <span class="op">=</span> (</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            softmax</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(d, <span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(np.exp(m <span class="op">-</span> m_new), <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            <span class="op">/</span> np.expand_dims(d_new, <span class="dv">1</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rescale and concat</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        softmax[:, i <span class="op">*</span> chunk_size : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> chunk_size] <span class="op">=</span> (</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            chunk_softmax</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(chunk_d, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(np.exp(chunk_m <span class="op">-</span> m_new), <span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            <span class="op">/</span> np.expand_dims(d_new, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 4. Update m_x and l_x from CURRENT global info.</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> m_new</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        d <span class="op">=</span> d_new</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> softmax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
</section>
</section>
<section id="flashattention" class="level2">
<h2 class="anchored" data-anchor-id="flashattention">3. FlashAttention</h2>
<p>有了 OnlineSoftmax 的概念，从 OnlineSoftmax 过渡到 FlashAttention 就很平滑了。</p>
<p>Attention 的计算核心就是用 <span class="math inline">\(Q\)</span> 和 <span class="math inline">\(K\)</span> 计算 Attention Score，用 Softmax 将 Attention Score 转换到概率空间，然后在 <span class="math inline">\(V\)</span> 做加权获取最终的 Attention 输出。标准的 Dot-Product Self Attention 如下所示：</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> np.matmul(Q, K.T)  <span class="co"># Dot-Product Attention</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> safe_softmax(S)  <span class="co"># (N, N)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> np.matmul(P, V)  <span class="co"># (N, d)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>从 Attention 端到端的计算上看，Softmax 的数值只是过程变量，且 Softmax 的数值最终呈现在 <span class="math inline">\(O\)</span> 里。在 <code>2.3</code> 小节中，我们知道 Softmax 的数值是可以分块在 Loop 中完成计算的，那么是否有可能在 <span class="math inline">\(O\)</span> 上面做 Online 更新呢？ —— 答案是必然的，<strong>有</strong>。</p>
<p>展开 FlashAttention 的计算细节之前我们先整体了解一下当前 GPU 的运算性能限制。</p>
<section id="compute-bound-vs-memory-bound" class="level3">
<h3 class="anchored" data-anchor-id="compute-bound-vs-memory-bound">3.1 Compute Bound Vs Memory Bound</h3>
<p>首先了解以下几个概念：</p>
<ul>
<li>算力 <span class="math inline">\(\pi\)</span>: 也称为计算平台的<strong>性能上限</strong>，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 <code>FLOPS</code> or <code>FLOP/s</code>。</li>
<li>带宽 <span class="math inline">\(\beta\)</span>: 也即计算平台的<strong>带宽上限</strong>，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是<code>Byte/s</code>。</li>
<li>计算强度上限 <span class="math inline">\(I_{max} = \frac{\pi}{\beta}\)</span>: 两个指标相除即可得到计算平台的<strong>计算强度上限</strong>。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是<code>FLOPs/Byte</code>。</li>
<li>模型的理论性能 <span class="math inline">\(P\)</span>: 我们最关心的指标，即模型在计算平台上<strong>实际</strong>所能达到的<strong>每秒浮点运算次数（理论值）</strong>。单位是<code>FLOPS</code> or <code>FLOP/s</code>。</li>
</ul>
<p>一个算法运行的效率是离不开硬件本身的。我们往往想知道：对于一个运算量为 <span class="math inline">\(\pi_t\)</span>，数据读取存储量为 <span class="math inline">\(\beta_t\)</span> 的算法，它在算力上限为 <span class="math inline">\(\pi\)</span>，带宽上限为 <span class="math inline">\(\beta\)</span> 的硬件上，能达到的最大性能 <span class="math inline">\(P\)</span> (Attanable Performance)是多少？这里最大性能 <span class="math inline">\(P\)</span> 指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是FLOP/s。</p>
<p><strong>Roof-line模型</strong>就是为了解答这一问题而提出的，它能直观帮我们看到算法在硬件上能跑得多快，模型见下图。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/15426dbd.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">15426dbd.png</figcaption>
</figure>
</div>
<p>如图，横坐标 <span class="math inline">\(I\)</span> 表示计算强度，满足 <span class="math inline">\(I=\frac{\pi_t}{\beta_t}\)</span> ；纵坐标 <span class="math inline">\(P\)</span> 表示算法运行在硬件上的性能。算法的运行性能<strong>不会超过硬件本身的计算上限</strong>，所以 <span class="math inline">\(P\)</span> 的最大值取到 <span class="math inline">\(\pi\)</span>。根据我们之前的分析，当 <span class="math inline">\(I&gt;\frac{\pi}{\beta}\)</span> 时，存在计算限制(Compute Bound)；当 <span class="math inline">\(I&lt;\frac{\pi}{\beta}\)</span> 时，存在内存限制(Memory Bound)。</p>
</section>
<section id="gpu-memory-layout" class="level3">
<h3 class="anchored" data-anchor-id="gpu-memory-layout">3.2 GPU Memory Layout</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/ace8b864.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">ace8b864.png</figcaption>
</figure>
</div>
<p>在讲计算之前首先了解 GPU 的存储细节。上图最左是 FlashAttention 论文所绘制的硬件不同的存储类型、存储大小和带宽。一般来说，GPU 上的存储分类，可以按照是否在芯片上分为<strong>片上内存(on chip)</strong>和<strong>片下内存(off chip)</strong>。</p>
<ul>
<li>片上内存：主要用于缓存（cache）及少量特殊存储单元（例如texture），其特点是<strong>存储空间小，但带宽大</strong>。对应到上图中，SRAM 就属于片上内存，它的存储空间只有 <code>20MB</code>(SM 数量*L1 Cache 容量)，但是带宽可以达到 <code>19TB/s</code>。</li>
<li>片下内存：主要用于全局存储（global memory），即我们常说的<strong>显存</strong>，其特点是<strong>存储空间大，但带宽小</strong>，对应到上图中，HBM 就属于片下内存，它的存储空间有 <code>40GB</code>(A100 40GB)，但带宽相比于 SRAM 就小得多，只有 <code>1.5TB/s</code>。</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/0cc7e3db.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">0cc7e3db.png</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/6826c3c0.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">6826c3c0.png</figcaption>
</figure>
</div>
<p>从上图的硬件架构上看，越是靠近 SM 核心的存储，读写速度就越快。上图中 L1 Cache 则是 FlashAttention 中的 SRAM，为每个 SM 独占，L2 Cache 是所有 SM 共享，再往下就是 HMB。所以在算法计算中，如果读写全部在 SRAM 上，将达到 IO 速度的上限，计算自然就快了。</p>
</section>
<section id="standard-attention" class="level3">
<h3 class="anchored" data-anchor-id="standard-attention">3.3 Standard Attention</h3>
<p>Given input sequences <span class="math inline">\(Q, K, V \in \mathbb{R}^{N \times d}\)</span> where <span class="math inline">\(N\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the head dimension, we want to compute the attention output <span class="math inline">\(O \in \mathbb{R}^{N \times d}\)</span>:</p>
<p><span class="math display">\[ S = QK^T \in \mathbb{R}^{N \times N}, P = softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d} \]</span></p>
<p>标准 Attention 的算法如下，所有中间结果储存在 HBM，空间复杂度最高的是 <span class="math inline">\(O(N^2)\)</span> 的 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span>，对于 HBM 的 IO(读写) 复杂度是 <span class="math inline">\(O(Nd+N^2)\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/7e5428b9.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">7e5428b9.png</figcaption>
</figure>
</div>
<p>Standard attention implementations materialize the matrices <span class="math inline">\(S\)</span> and <span class="math inline">\(P\)</span> to HBM, which <strong>takes <span class="math inline">\(O(N^2)\)</span> memory</strong>. Often <span class="math inline">\(N\gg d\)</span> (e.g., for GPT2, <span class="math inline">\(N=1024\)</span> and <span class="math inline">\(d=64\)</span>(<strong>single head dim</strong>)). We describe the standard attention implementation in Algorithm <code>0</code>. As some or most of the operations are <strong>memory-bound</strong> (e.g., softmax), the large number of memory accesses translates to slow wall-clock time.</p>
<p>This problem is exacerbated(恶化) by other <strong>elementwise operations</strong> applied to the attention matrix, such as masking applied to <span class="math inline">\(S\)</span> or dropout applied to <span class="math inline">\(P\)</span>.</p>
</section>
<section id="flashattention-1" class="level3">
<h3 class="anchored" data-anchor-id="flashattention-1">3.4 FlashAttention</h3>
<p>FlashAttention 的核心理念是尽可能少读写 HBM，在具体实现上采用了 <strong>tiling</strong>（分块）和 recompute（重新计算 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span>，因为 forward 过程中不存储 <span class="math inline">\(O(N^2)\)</span> 的 Attention Scores，但是反向需要用到，所以采用了重新计算。）</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/718d7308.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">718d7308.png</figcaption>
</figure>
</div>
<p>同样对 FlashAttention 进行 IO 复杂度分析，<span class="math inline">\(M\)</span> 为单个流式处理器的 L1 Cache（大小约100KB，A100 中有 108 个 SM，每个 SM L1 Cache 是 192KB，总计约 20MB），<span class="math inline">\(d\)</span> 是 single head dim（一般是64/128）。</p>
<p>首先我们看 Outer Loop，一次 Loop 会访问一遍完整的 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span>，IO 复杂度是 <span class="math inline">\(O(Nd)\)</span>。 对于 Inner Loop，一次 Loop 会访问一遍完整的 <span class="math inline">\(Q\)</span>，为 <span class="math inline">\(O(Nd)\)</span>；Inner Loop 的次数 <span class="math inline">\(T_c = \lceil{\frac{N}{B_c}}\rceil\)</span>，为 <span class="math inline">\(O(NdM^{-1})\)</span>，相乘可以得到 Inner Loop 的 IO 复杂度是 <span class="math inline">\(O(N^2d^2M^{-1})\)</span>，因此 FlashAttention 算法整体的 IO 复杂度为 <span class="math inline">\(O(Nd + N^2d^2M^{-1})\)</span>。</p>
<p>对比标准 Attention 的 <span class="math inline">\(O(Nd+N^2)\)</span>，因为 <span class="math inline">\(\frac{d^2}{M}\ll1\)</span>，因此对于 HBM 的 IO 复杂度得到了显著的降低。此外，再加上 kernel fusion，进一步对计算加速。</p>
<p>同时，因为不需要存储 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span>（<span class="math inline">\(O(N^2)\)</span>），<span class="math inline">\(m\)</span> 和 <span class="math inline">\(l\)</span> 的开销小（<span class="math inline">\(O(N)\)</span>），空间复杂度从标准的 <span class="math inline">\(O(N^2d)\)</span> 降低到了 <span class="math inline">\(O(Nd)\)</span>。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/c8105e32.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">c8105e32.png</figcaption>
</figure>
</div>
<p>注意上图左侧中，Linformer Attention 和 OpenAI Sparse Attention 不是 <strong>exact</strong> Attention。</p>
</section>
<section id="代码实现-1" class="level3">
<h3 class="anchored" data-anchor-id="代码实现-1">3.5 代码实现</h3>
<p>沿用 Online Softmax 的框架，并从 1d 的 chunk-wise 拓展到 2d 的 block-wise。模型一个 SM 单元中的计算逻辑，此处用 numpy 验证其准确性，无加速效果，只减少了空间复杂度。简单起见忽略了 mask，dropout等细节。</p>
<p>关于 backward 的计算，感兴趣的可以自行查看论文，核心思想是重新计算 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span>，把前向中没有存储的变量进行 recompute，搭回反向的通道。</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attention(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    Q: np.ndarray, K: np.ndarray, V: np.ndarray, Br: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>, Bc: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    N, d <span class="op">=</span> Q.shape</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> np.zeros_like(Q)  <span class="co"># Alloc memory for `O` in HBM</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    outer_loop_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> Bc))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    inner_loop_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> Br))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    m_x_all <span class="op">=</span> np.zeros((N))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    l_x_all <span class="op">=</span> np.zeros((N))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(outer_loop_steps):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(inner_loop_steps):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get chunk data</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            chunk_q <span class="op">=</span> Q[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]  <span class="co"># (Br, d)</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            chunk_k <span class="op">=</span> K[j <span class="op">*</span> Bc : (j <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Bc]  <span class="co"># (Bc, d)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            chunk_v <span class="op">=</span> V[j <span class="op">*</span> Bc : (j <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Bc]  <span class="co"># (Bc, d)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            m_x <span class="op">=</span> m_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            l_x <span class="op">=</span> l_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 1. Perform attention on block data.</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            chunk_s <span class="op">=</span> np.matmul(chunk_q, chunk_k.T)  <span class="co"># (Br, Bc)</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            chunk_m_x <span class="op">=</span> np.<span class="bu">max</span>(chunk_s, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            chunk_f_x <span class="op">=</span> np.exp(chunk_s <span class="op">-</span> np.expand_dims(chunk_m_x, <span class="dv">1</span>))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            chunk_l_x <span class="op">=</span> np.<span class="bu">sum</span>(chunk_f_x, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            chunk_p <span class="op">=</span> chunk_f_x <span class="op">/</span> np.expand_dims(chunk_l_x, <span class="dv">1</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            chunk_o <span class="op">=</span> np.matmul(chunk_p, chunk_v)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 2. Update `GLOBAL` stats.</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            m_x_new <span class="op">=</span> np.maximum(m_x, chunk_m_x)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            l_x_new <span class="op">=</span> l_x <span class="op">*</span> np.exp(m_x <span class="op">-</span> m_x_new) <span class="op">+</span> chunk_l_x <span class="op">*</span> np.exp(</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>                chunk_m_x <span class="op">-</span> m_x_new</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 3. Rescale and update `GLOBAL` output.</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Rescale(Old)</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> (</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>                O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(l_x, <span class="dv">1</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(np.exp(m_x <span class="op">-</span> m_x_new), <span class="dv">1</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> np.expand_dims(l_x_new, <span class="dv">1</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update(New)</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">+=</span> (</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>                chunk_o</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(np.exp(chunk_m_x <span class="op">-</span> m_x_new), <span class="dv">1</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> np.expand_dims(l_x_new, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>            m_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> m_x_new</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>            l_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> l_x_new</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
</section>
</section>
<section id="pagedattention" class="level2">
<h2 class="anchored" data-anchor-id="pagedattention">4. PagedAttention</h2>
<p>PagedAttention 的核心是提升显存利用效率，减少存储碎片，最终达到提升服务吞吐量的效果，不同于 FlashAttention，它对单一的序列生成理论上没有加速效果。</p>
<section id="kv-cache" class="level3">
<h3 class="anchored" data-anchor-id="kv-cache">4.1 KV Cache</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/24972e88.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">24972e88.png</figcaption>
</figure>
</div>
<p>First, the existing systems suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they <strong>pre-allocate</strong> a contiguous chunk of memory with the request’s maximum length(e.g., 2048 tokens).</p>
<p>LLM 模型在做推理时，会对 KV Cache 进行提前的显存分配。因为解码前不知道一个序列最终长度是多少（等待 EOS 或者最长时截断），模型会根据用户传参（max_length）来分配一段最大的<strong>连续</strong>的显存空间，等待自回归解码时逐个填充（此处用 greedy decoding and sampling 来举例，如果是其他解码方案，显存的分配会更多）。</p>
</section>
<section id="memory-fragmentation" class="level3">
<h3 class="anchored" data-anchor-id="memory-fragmentation">4.2 Memory Fragmentation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/c7d061fa.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">c7d061fa.png</figcaption>
</figure>
</div>
<p>上图展示了三种不同的显存浪费的情况：</p>
<ul>
<li>reserved: 当前解码时刻还未用到的显存(但是后面将会使用，假设我们已经知道输出序列。)</li>
<li>internal fragmentation: 上面提到的根据 max_length 提前分配的显存，在 batch 推理的过程中，这部分将持续占用，但不产生任何价值。</li>
<li>external fragmentation: 推理框架（文中举例是 FT 和 Orca），在为多个序列分配各自的连续的显存空间存在的碎片区域。</li>
</ul>
</section>
<section id="pagedattention-1" class="level3">
<h3 class="anchored" data-anchor-id="pagedattention-1">4.3 PagedAttention</h3>
<p>In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is:</p>
<ul>
<li>Large: Takes up to 1.7GB for a single sequence in LLaMA-13B.</li>
<li>Dynamic: Its size depends on the sequence length, which is highly variable and unpredictable. As a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste 60% – 80% of memory due to fragmentation and over-reservation.</li>
</ul>
<p>To address this problem, we introduce <strong>PagedAttention</strong>, an attention algorithm inspired by the classic idea of <strong>virtual memory and paging in operating systems</strong>. Unlike the traditional attention algorithms, PagedAttention <strong>allows storing continuous keys and values in non-contiguous memory space</strong>. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.</p>
<section id="memory-block" class="level4">
<h4 class="anchored" data-anchor-id="memory-block">4.3.1 Memory Block</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/annimation0.gif" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">annimation0.gif</figcaption>
</figure>
</div>
<p>上图演示的是最新一个 token <code>for</code> 对于已有的序列（可以是输入的 prompt）<code>Alan Turing is a computer scientist and mathematician renowned for</code> 做 Attention 的过程。图示中以 Block 分块管理显存，以达到用<strong>不连续</strong>代替连续的显存空间。</p>
<p>PagedAttention 想要达到最小化的显存浪费，在上文中介绍的 <code>reserved</code> 和 <code>external fragmentation</code>在这种方式下可以完全清除，但 <code>internal fragmentation</code> 无法完全消除。图中 Block 的 size 为 4，在 Block 2 中存在 2 个单元的碎片浪费。</p>
</section>
<section id="block-table" class="level4">
<h4 class="anchored" data-anchor-id="block-table">4.3.2 Block table</h4>
<p>Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous <strong>logical blocks</strong> of a sequence are mapped to non-contiguous <strong>physical blocks</strong> via a <strong>block table</strong>. The physical blocks are allocated on demand as new tokens are generated.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/annimation1.gif" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">annimation1.gif</figcaption>
</figure>
</div>
<p>PagedAttention 在实现上采用了 KV Cache Manager 维护了多张 Block table，表里存着 logical blocks 到 phsical blocks 的映射。并且，为了尽量消除 <code>internal fragmentation</code>，他实现了动态的显存分配管理。</p>
</section>
<section id="memory-sharing" class="level4">
<h4 class="anchored" data-anchor-id="memory-sharing">4.3.3 Memory Sharing</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/annimation2.gif" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">annimation2.gif</figcaption>
</figure>
</div>
<p>在 LLM 解码过程中，我们可能会采取多种解码方式扩大搜索空间，如 Parallel sampling、Beam Search 等。此类解码方式将会输出多个 candidates。有个直观的感受是，在多序列生成过程中，大概率会以一个<strong>树状</strong>的样式增长，因此有可能可以共享某些 KV Cache。然而在传统解码模式下，因为 KV Cache 分开存放在连续的显存空间，显存的占用将会线性增长，无法达成 KV Cache 的共享。</p>
<p>PagedAttention 以引用计数和 Copy-on-Write 的特性实现了多序列生成的 KV Cache 共享具体，Parallel sampling 的解码流程如下图所示：</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../attachments/annimation3.gif" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">annimation3.gif</figcaption>
</figure>
</div>
<p>在 Beam Search 的场景下，则会管理一个树状结构，在最新时刻的解码结果出现后，可以将废弃的 Block 进行剪枝回收。如下图示例，Beam width 为4，虚线左侧是 <span class="math inline">\(t-1\)</span> 时刻的 KV Cache 占用情况，当 <span class="math inline">\(t\)</span> 时刻解码完成后，最新的 Block9~12 依赖的 KV Cache 全部来自 Block6 和 Block7，则 Block2、4、5、8 的显存将被回收。</p>
<p>../attachments <img src="../attachments/fc7f0992.png" class="img-fluid" style="width:60.0%" alt="fc7f0992.png"></p>
<p>除此之外，在大批量多序列推理的过程中，因为显存的动态分配，在某个时刻有可能会有显存不足的情况，为了防止此类情况，PagedAttention 实现了内存抢占机制，核心思想是 the <strong>earliest</strong> arrived requests are <strong>served first</strong> and the <strong>latest</strong> requests are <strong>preempted first</strong>。具体实现方式有:</p>
<ul>
<li>Swapping: latest requests 占用的 Block 将会被放入 CPU RAM，等待前方解码完成释放空间后，再从 CPU RAM 中重新取回显存。</li>
<li>Recomputation: 直接释放 latest requests 占用的 Block，后续将重新计算得到 KV Cache（在这里重新计算是很快的，因为我们已知当前所有的 token，直接用 Masked Attention并行计算）。</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="抛砖引玉" class="level2">
<h2 class="anchored" data-anchor-id="抛砖引玉">5. 抛砖引玉</h2>
<p>此次分享专注的是 Transformer 在计算和推理上结构优化，该类方法主要通过优化 Transformer 的结构以实现推理性能的提升。</p>
<p>当前 LLM 加速技术是热门的研究方向，目前包括但不限于以下几点(<a href="https://zhuanlan.zhihu.com/p/642412124">NLP（十八）：LLM 的推理优化技术纵览</a>)：</p>
<ul>
<li>子图融合（subgraph fusion）</li>
<li>模型压缩（Model Compression）</li>
<li>并行化（Parallelism）</li>
<li>Transformer 结构优化</li>
<li>动态批处理（Dynamic Batch, Continuous batch）</li>
<li>KV cache 优化</li>
<li>投机采样与MoE</li>
</ul>
<p><strong>一起学习、讨论和进步。</strong></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "FlashAttention 和 PagedAttention"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## 0. 参考文献</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>[<span class="co">[</span><span class="ot">8 May 2018</span><span class="co">]</span>Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>[<span class="co">[</span><span class="ot">27 May 2022</span><span class="co">]</span>FlashAttention: Fast and Memory-Efficient Exact Attention</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>with IO-Awareness](https://arxiv.org/abs/2205.14135)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>[<span class="co">[</span><span class="ot">17 Jul 2023</span><span class="co">]</span>FlashAttention-2:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>[<span class="co">[</span><span class="ot">12 Sep 2023</span><span class="co">]</span>Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">NLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能</span><span class="co">](https://zhuanlan.zhihu.com/p/638468472)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">NLP（十八）：LLM 的推理优化技术纵览</span><span class="co">](https://zhuanlan.zhihu.com/p/642412124)</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</span><span class="co">](https://blog.vllm.ai/2024/06/20/vllm.html)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">FlashAttention 的速度优化原理是怎样的？</span><span class="co">](https://www.zhihu.com/question/611236756/answer/3132304304)</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">一心二用的Online Softmax</span><span class="co">](https://zhuanlan.zhihu.com/p/638788074)</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎</span><span class="co">](https://zhuanlan.zhihu.com/p/669926191)</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1. Overview</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>|Term | 是什么| 用在哪里| 技术亮点|</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>|:---|:---|:---|:---|</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>|Softmax | - | - | - |</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>|SafeSoftmax | 为了防止 Softmax 计算过程中 exp 函数出现数值上下溢的情况，提出的一种数值处理方案 | Torch,TF,Paddle 等框架，嵌入至绝大多数模型 | 数值处理，指数部分减去原始向量的 max 值。 |</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>|OnlineSoftmax | Softmax 的一种在线更新的计算方法 | 任何需要计算 Softmax 的场景 | 降低对原始向量的遍历次数，计算速度加快。 |</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>|FlashAttention | 一种 **精确的**（非近似） Attention 的加速算法 | 所有用到 Attention OP 的模型，包括不限于类 Bert，类 GPT 等模型。 | 结合 GPU Memory 的硬件 Layout，基于 Online Softmax 对 Attention OP 进行 **精细化** 优化。不降 FLOPs，降低 HBM 读写次数和算子融合达到加速效果。&lt;br&gt;时间复杂度 **不变**，空间复杂度从 $O(N^2)$ 降低至 $O(N)$。 |</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>|FlashAttention-2 | 基于 FlashAttention 算法，提升 FlashAttention 的计算效率 | 与 FlashAttention 一致 | FlashAttention 计算提效，提升了非 matmul 的 FLOPs，引入了更好的并行化计算方式和内存复用。 |</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>|PagedAttention | 一种提升模型服务 **吞吐量** 的算法，提升批量计算时显存的利用效率 | vLLM 加速框架的核心技术，用于各种用到用到 Decoder 模型的服务中，适用于市面上各类 LLM | LLM 推理框架（如 FT 和 Orca）在批量推理时，由于用于存储 KV Cache 的显存需要提前分配，并针对每一个序列分配在连续的显存空间上，导致存在大量的碎片（存在于 **序列内** 和 **序列间**）。<span class="kw">&lt;br&gt;</span>PagedAttention 引入内存分页管理的思想，用一个 Scheduler 单例，以 Block 为单位动态地分配显存，提升显存利用率（Batch 计算中复用，Decode 过程中剪枝等），从而加速批量计算，提升模型服务的吞吐量。 |</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>: {tbl-colwidths="<span class="co">[</span><span class="ot">10, 20, 20, 50</span><span class="co">]</span>"}</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2. OnlineSoftmax</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.1 Softmax</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>$$ y = softmax(x) $$</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^{V} e^{x_j}}  \tag{1}$$</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>where $x, y \in \mathbb{R}^V$   . The naive implementation (see algorithm <span class="in">`1`</span>) scans the input vector two times - one to calculate the normalization term $d_V$ and another to compute output values $y_i$ - effectively doing **three memory accesses** per vector element: two loads and one store.</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="al">![d20eb1b8.png](../attachments/d20eb1b8.png)</span>{width=70%}</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.2 SafeSoftmax</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>Unfortunately, on real hardware, where the range of numbers represented is limited, the line <span class="in">`3`</span> of the algorithm <span class="in">`1`</span> can **overflow or underflow due to the exponent**. There is a safe form of <span class="in">`(1)`</span>, which is immune to this problem:</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>$$ y_i = \frac{e^{x_i-\max_{k=1}^{V}x_k}}{\sum_{j=1}^{V} e^{x_j-\max_{k=1}^{V}x_k}}  \tag{2}$$</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="al">![de4cc3d9.png](../attachments/de4cc3d9.png)</span>{width=70%}</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>But Safe Softmax does three passes over input vector: The first one calculates the maximum value $m_V$ , the second one - normalization term $d_V$ , and the third one - final values $y_i$, see algorithm <span class="in">`2`</span>; This results in **4 memory access** per vector element overall.</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.3 OnlineSoftmax</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>The algorithm <span class="in">`3`</span> calculates both the maximum value m and the normalization term $d$ in a **single pass** over input vector with negligible additional cost of two operations per vector element. It **reduces memory accesses** from 4 down to 3 per vector element for the Softmax function evaluation. Inspiration came from the numerically stable variance calculation online algorithm.</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="al">![ba540d94.png](../attachments/ba540d94.png)</span>{width=70%}</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>Essentially, the algorithm keeps the maximum value $m$ and the normalization term $d$ as it iterates over elements of the input array. At each iteration it **needs to adjust** the normalizer $d$ to the new maximum $m_j$ and only then add new value to the normalizer.</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>这里需要注意的点在于，$m_j$ 和 $d_j$ 是在同一遍 Loop 中实时更新的，$m_j$ 求 max，很容易更新；$d_j$ 是指数项求和，在 Loop 中一旦发生最大值更新，即 $m_j \ne m_{j-1}$ ，$d_{j-1}$ 则需要进行一个指数项的缩放，其**数值**为 $e^{m_{j-1}-m_j}$。</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>实际上这里还可以继续优化，把 $y$ 的计算也放在第一遍 Loop 中，如果这么做，每一次 iter 需要更新的值是 $<span class="sc">\{</span>y_1, y_2...y_{j-1}<span class="sc">\}</span>$，然后拼接上最新的 ${y_j}$，缩放的方式需要同时考虑分子的 $m$ 和分母中的 $v$，其**数值**为 $\frac{d_{j-1} * e^{m_{j-1}-m_j}}{d_j}$。(注意针对 $y$ 的缩放和针对 $d$ 的缩放的差异)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.4 代码实现</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>OnlineSoftmax实现，如 <span class="in">`2.3`</span> 最后的结论，用单 Loop 完成 SafeSoftmax 的数值计算，且支持 chunk 内并行。</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> online_softmax(x: np.ndarray, chunk_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>    num_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(x.shape[<span class="dv">1</span>] <span class="op">/</span> chunk_size))</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>]))</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>]))</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    softmax <span class="op">=</span> np.zeros_like(x)</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get chunk data</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        chunk_x <span class="op">=</span> x[:, i <span class="op">*</span> chunk_size : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> chunk_size]</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 1. Perform local softmax on chunk data.</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>        chunk_m <span class="op">=</span> np.<span class="bu">max</span>(chunk_x, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>        chunk_f <span class="op">=</span> np.exp(chunk_x <span class="op">-</span> np.expand_dims(chunk_m, <span class="dv">1</span>))</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>        chunk_d <span class="op">=</span> np.<span class="bu">sum</span>(chunk_f, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>        chunk_softmax <span class="op">=</span> chunk_f <span class="op">/</span> np.expand_dims(chunk_d, <span class="dv">1</span>)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 2. Get new m_x and l_x.</span></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>        m_new <span class="op">=</span> np.maximum(m, chunk_m)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        d_new <span class="op">=</span> d <span class="op">*</span> np.exp(m <span class="op">-</span> m_new) <span class="op">+</span> chunk_d <span class="op">*</span> np.exp(chunk_m <span class="op">-</span> m_new)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 3. Rescale(OLD + NEW) and concat(NEW) softmax values.</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rescale</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>        softmax <span class="op">=</span> (</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>            softmax</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(d, <span class="dv">1</span>)</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(np.exp(m <span class="op">-</span> m_new), <span class="dv">1</span>)</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            <span class="op">/</span> np.expand_dims(d_new, <span class="dv">1</span>)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rescale and concat</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>        softmax[:, i <span class="op">*</span> chunk_size : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> chunk_size] <span class="op">=</span> (</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>            chunk_softmax</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(chunk_d, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> np.expand_dims(np.exp(chunk_m <span class="op">-</span> m_new), <span class="dv">1</span>)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>            <span class="op">/</span> np.expand_dims(d_new, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># INFO: 4. Update m_x and l_x from CURRENT global info.</span></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> m_new</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>        d <span class="op">=</span> d_new</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> softmax</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3. FlashAttention</span></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>有了 OnlineSoftmax 的概念，从 OnlineSoftmax 过渡到 FlashAttention 就很平滑了。</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>Attention 的计算核心就是用 $Q$ 和 $K$ 计算 Attention Score，用 Softmax 将 Attention Score 转换到概率空间，然后在 $V$ 做加权获取最终的 Attention 输出。标准的 Dot-Product Self Attention 如下所示：</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> np.matmul(Q, K.T)  <span class="co"># Dot-Product Attention</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> safe_softmax(S)  <span class="co"># (N, N)</span></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> np.matmul(P, V)  <span class="co"># (N, d)</span></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>从 Attention 端到端的计算上看，Softmax 的数值只是过程变量，且 Softmax 的数值最终呈现在 $O$ 里。在 <span class="in">`2.3`</span> 小节中，我们知道 Softmax 的数值是可以分块在 Loop 中完成计算的，那么是否有可能在 $O$ 上面做 Online 更新呢？ —— 答案是必然的，**有**。</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>展开 FlashAttention 的计算细节之前我们先整体了解一下当前 GPU 的运算性能限制。</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.1 Compute Bound Vs Memory Bound</span></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>首先了解以下几个概念：</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>算力 $\pi$: 也称为计算平台的**性能上限**，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 <span class="in">`FLOPS`</span> or <span class="in">`FLOP/s`</span>。</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>带宽 $\beta$: 也即计算平台的**带宽上限**，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是<span class="in">`Byte/s`</span>。</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>计算强度上限 $I_{max} = \frac{\pi}{\beta}$: 两个指标相除即可得到计算平台的**计算强度上限**。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是<span class="in">`FLOPs/Byte`</span>。</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>模型的理论性能 $P$: 我们最关心的指标，即模型在计算平台上**实际**所能达到的**每秒浮点运算次数（理论值）**。单位是<span class="in">`FLOPS`</span> or <span class="in">`FLOP/s`</span>。</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>一个算法运行的效率是离不开硬件本身的。我们往往想知道：对于一个运算量为 $\pi_t$，数据读取存储量为 $\beta_t$ 的算法，它在算力上限为 $\pi$，带宽上限为 $\beta$ 的硬件上，能达到的最大性能 $P$ (Attanable Performance)是多少？这里最大性能 $P$ 指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是FLOP/s。</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>**Roof-line模型**就是为了解答这一问题而提出的，它能直观帮我们看到算法在硬件上能跑得多快，模型见下图。</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a><span class="al">![15426dbd.png](../attachments/15426dbd.png)</span>{width=70%}</span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>如图，横坐标 $I$ 表示计算强度，满足 $I=\frac{\pi_t}{\beta_t}$ ；纵坐标 $P$ 表示算法运行在硬件上的性能。算法的运行性能**不会超过硬件本身的计算上限**，所以 $P$ 的最大值取到 $\pi$。根据我们之前的分析，当 $I&gt;\frac{\pi}{\beta}$ 时，存在计算限制(Compute Bound)；当 $I&lt;\frac{\pi}{\beta}$ 时，存在内存限制(Memory Bound)。</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.2 GPU Memory Layout</span></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="al">![ace8b864.png](../attachments/ace8b864.png)</span>{width=80%}</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>在讲计算之前首先了解 GPU 的存储细节。上图最左是 FlashAttention 论文所绘制的硬件不同的存储类型、存储大小和带宽。一般来说，GPU 上的存储分类，可以按照是否在芯片上分为**片上内存(on chip)**和**片下内存(off chip)**。</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>片上内存：主要用于缓存（cache）及少量特殊存储单元（例如texture），其特点是**存储空间小，但带宽大**。对应到上图中，SRAM 就属于片上内存，它的存储空间只有 <span class="in">`20MB`</span>(SM 数量<span class="sc">\*</span>L1 Cache 容量)，但是带宽可以达到 <span class="in">`19TB/s`</span>。</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>片下内存：主要用于全局存储（global memory），即我们常说的**显存**，其特点是**存储空间大，但带宽小**，对应到上图中，HBM 就属于片下内存，它的存储空间有 <span class="in">`40GB`</span>(A100 40GB)，但带宽相比于 SRAM 就小得多，只有 <span class="in">`1.5TB/s`</span>。</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a><span class="al">![0cc7e3db.png](../attachments/0cc7e3db.png)</span>{width=50%}</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a><span class="al">![6826c3c0.png](../attachments/6826c3c0.png)</span>{width=70%}</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>从上图的硬件架构上看，越是靠近 SM 核心的存储，读写速度就越快。上图中 L1 Cache 则是 FlashAttention 中的 SRAM，为每个 SM 独占，L2 Cache 是所有 SM 共享，再往下就是 HMB。所以在算法计算中，如果读写全部在 SRAM 上，将达到 IO 速度的上限，计算自然就快了。</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.3 Standard Attention</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>Given input sequences $Q, K, V \in \mathbb{R}^{N \times d}$ where $N$ is the sequence length and $d$ is the head dimension, we want to compute the attention output $O \in \mathbb{R}^{N \times d}$:</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>$$ S = QK^T \in \mathbb{R}^{N \times N}, P = softmax(S) \in \mathbb{R}^{N \times N}, O = PV \in \mathbb{R}^{N \times d} $$</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>标准 Attention 的算法如下，所有中间结果储存在 HBM，空间复杂度最高的是 $O(N^2)$ 的 $S$ 和 $P$，对于 HBM 的 IO(读写) 复杂度是 $O(Nd+N^2)$</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a><span class="al">![7e5428b9.png](../attachments/7e5428b9.png)</span>{width=70%}</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>Standard attention implementations materialize the matrices $S$ and $P$ to HBM, which **takes $O(N^2)$ memory**. Often $N\gg d$ (e.g., for GPT2, $N=1024$ and $d=64$(**single head dim**)). We describe the standard attention implementation in Algorithm `0`. As some or most of the operations are **memory-bound** (e.g., softmax), the large number of memory accesses translates to slow wall-clock time.</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>This problem is exacerbated(恶化) by other **elementwise operations** applied to the attention matrix, such as masking applied to $S$ or dropout applied to $P$. </span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.4 FlashAttention</span></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>FlashAttention 的核心理念是尽可能少读写 HBM，在具体实现上采用了 **tiling**（分块）和 recompute（重新计算 $S$ 和 $P$，因为 forward 过程中不存储 $O(N^2)$ 的 Attention Scores，但是反向需要用到，所以采用了重新计算。）</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a><span class="al">![718d7308.png](../attachments/718d7308.png)</span>{width=80%}</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>同样对 FlashAttention 进行 IO 复杂度分析，$M$ 为单个流式处理器的 L1 Cache（大小约100KB，A100 中有 108 个 SM，每个 SM L1 Cache 是 192KB，总计约 20MB），$d$ 是 single head dim（一般是64/128）。</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>首先我们看 Outer Loop，一次 Loop 会访问一遍完整的 $K$ 和 $V$，IO 复杂度是 $O(Nd)$。</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>对于 Inner Loop，一次 Loop 会访问一遍完整的 $Q$，为 $O(Nd)$；Inner Loop 的次数 $T_c = \lceil{\frac{N}{B_c}}\rceil$，为 $O(NdM^{-1})$，相乘可以得到 Inner Loop 的 IO 复杂度是 $O(N^2d^2M^{-1})$，因此 FlashAttention 算法整体的 IO 复杂度为 $O(Nd + N^2d^2M^{-1})$。</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>对比标准 Attention 的 $O(Nd+N^2)$，因为 $\frac{d^2}{M}\ll1$，因此对于 HBM 的 IO 复杂度得到了显著的降低。此外，再加上 kernel fusion，进一步对计算加速。</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>同时，因为不需要存储 $S$ 和 $P$（$O(N^2)$），$m$ 和 $l$ 的开销小（$O(N)$），空间复杂度从标准的 $O(N^2d)$ 降低到了 $O(Nd)$。</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a><span class="al">![c8105e32.png](../attachments/c8105e32.png)</span>{width=80%}</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>注意上图左侧中，Linformer Attention 和 OpenAI Sparse Attention 不是 **exact** Attention。</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.5 代码实现</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>沿用 Online Softmax 的框架，并从 1d 的 chunk-wise 拓展到 2d 的 block-wise。模型一个 SM 单元中的计算逻辑，此处用 numpy 验证其准确性，无加速效果，只减少了空间复杂度。简单起见忽略了 mask，dropout等细节。</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>关于 backward 的计算，感兴趣的可以自行查看论文，核心思想是重新计算 $S$ 和 $P$，把前向中没有存储的变量进行 recompute，搭回反向的通道。</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attention(</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>    Q: np.ndarray, K: np.ndarray, V: np.ndarray, Br: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>, Bc: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>    N, d <span class="op">=</span> Q.shape</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>    O <span class="op">=</span> np.zeros_like(Q)  <span class="co"># Alloc memory for `O` in HBM</span></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>    outer_loop_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> Bc))</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>    inner_loop_steps <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> Br))</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>    m_x_all <span class="op">=</span> np.zeros((N))</span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>    l_x_all <span class="op">=</span> np.zeros((N))</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(outer_loop_steps):</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(inner_loop_steps):</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get chunk data</span></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>            chunk_q <span class="op">=</span> Q[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]  <span class="co"># (Br, d)</span></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>            chunk_k <span class="op">=</span> K[j <span class="op">*</span> Bc : (j <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Bc]  <span class="co"># (Bc, d)</span></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>            chunk_v <span class="op">=</span> V[j <span class="op">*</span> Bc : (j <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Bc]  <span class="co"># (Bc, d)</span></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>            m_x <span class="op">=</span> m_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>            l_x <span class="op">=</span> l_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 1. Perform attention on block data.</span></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>            chunk_s <span class="op">=</span> np.matmul(chunk_q, chunk_k.T)  <span class="co"># (Br, Bc)</span></span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>            chunk_m_x <span class="op">=</span> np.<span class="bu">max</span>(chunk_s, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>            chunk_f_x <span class="op">=</span> np.exp(chunk_s <span class="op">-</span> np.expand_dims(chunk_m_x, <span class="dv">1</span>))</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>            chunk_l_x <span class="op">=</span> np.<span class="bu">sum</span>(chunk_f_x, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>            chunk_p <span class="op">=</span> chunk_f_x <span class="op">/</span> np.expand_dims(chunk_l_x, <span class="dv">1</span>)</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>            chunk_o <span class="op">=</span> np.matmul(chunk_p, chunk_v)</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 2. Update `GLOBAL` stats.</span></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>            m_x_new <span class="op">=</span> np.maximum(m_x, chunk_m_x)</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>            l_x_new <span class="op">=</span> l_x <span class="op">*</span> np.exp(m_x <span class="op">-</span> m_x_new) <span class="op">+</span> chunk_l_x <span class="op">*</span> np.exp(</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>                chunk_m_x <span class="op">-</span> m_x_new</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>            <span class="co"># INFO: 3. Rescale and update `GLOBAL` output.</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Rescale(Old)</span></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>            O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> (</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>                O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br]</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(l_x, <span class="dv">1</span>)</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(np.exp(m_x <span class="op">-</span> m_x_new), <span class="dv">1</span>)</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> np.expand_dims(l_x_new, <span class="dv">1</span>)</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update(New)</span></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>            O[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">+=</span> (</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>                chunk_o</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> np.expand_dims(np.exp(chunk_m_x <span class="op">-</span> m_x_new), <span class="dv">1</span>)</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> np.expand_dims(l_x_new, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>            m_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> m_x_new</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>            l_x_all[i <span class="op">*</span> Br : (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> Br] <span class="op">=</span> l_x_new</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> O</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4. PagedAttention</span></span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>PagedAttention 的核心是提升显存利用效率，减少存储碎片，最终达到提升服务吞吐量的效果，不同于 FlashAttention，它对单一的序列生成理论上没有加速效果。</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.1 KV Cache</span></span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a><span class="al">![24972e88.png](../attachments/24972e88.png)</span>{width=50%}</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>First, the existing systems suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they **pre-allocate** a contiguous chunk of memory with the request’s maximum length(e.g., 2048 tokens).</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>LLM 模型在做推理时，会对 KV Cache 进行提前的显存分配。因为解码前不知道一个序列最终长度是多少（等待 EOS 或者最长时截断），模型会根据用户传参（max_length）来分配一段最大的**连续**的显存空间，等待自回归解码时逐个填充（此处用 greedy decoding and sampling 来举例，如果是其他解码方案，显存的分配会更多）。</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.2 Memory Fragmentation</span></span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a><span class="al">![c7d061fa.png](../attachments/c7d061fa.png)</span></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>上图展示了三种不同的显存浪费的情况：</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>reserved: 当前解码时刻还未用到的显存(但是后面将会使用，假设我们已经知道输出序列。)</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>internal fragmentation: 上面提到的根据 max_length 提前分配的显存，在 batch 推理的过程中，这部分将持续占用，但不产生任何价值。</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>external fragmentation: 推理框架（文中举例是 FT 和 Orca），在为多个序列分配各自的连续的显存空间存在的碎片区域。</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.3 PagedAttention</span></span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is:</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large: Takes up to 1.7GB for a single sequence in LLaMA-13B.</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dynamic: Its size depends on the sequence length, which is highly variable and unpredictable. As a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste 60% – 80% of memory due to fragmentation and over-reservation.</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>To address this problem, we introduce **PagedAttention**, an attention algorithm inspired by the classic idea of **virtual memory and paging in operating systems**. Unlike the traditional attention algorithms, PagedAttention **allows storing continuous keys and values in non-contiguous memory space**. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4.3.1 Memory Block</span></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a><span class="al">![annimation0.gif](../attachments/annimation0.gif)</span>{width=70%}</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>上图演示的是最新一个 token <span class="in">`for`</span> 对于已有的序列（可以是输入的 prompt）<span class="in">`Alan Turing is a computer scientist and mathematician renowned for`</span> 做 Attention 的过程。图示中以 Block 分块管理显存，以达到用**不连续**代替连续的显存空间。</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>PagedAttention 想要达到最小化的显存浪费，在上文中介绍的 <span class="in">`reserved`</span> 和 <span class="in">`external fragmentation`</span>在这种方式下可以完全清除，但 <span class="in">`internal fragmentation`</span> 无法完全消除。图中 Block 的 size 为 4，在 Block 2 中存在 2 个单元的碎片浪费。</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4.3.2  Block table</span></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous **logical blocks** of a sequence are mapped to non-contiguous **physical blocks** via a **block table**. The physical blocks are allocated on demand as new tokens are generated.</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a><span class="al">![annimation1.gif](../attachments/annimation1.gif)</span>{width=70%}</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>PagedAttention 在实现上采用了 KV Cache Manager 维护了多张 Block table，表里存着 logical blocks 到 phsical blocks 的映射。并且，为了尽量消除 <span class="in">`internal fragmentation`</span>，他实现了动态的显存分配管理。</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4.3.3 Memory Sharing</span></span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a><span class="al">![annimation2.gif](../attachments/annimation2.gif)</span>{width=70%}</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>在 LLM 解码过程中，我们可能会采取多种解码方式扩大搜索空间，如 Parallel sampling、Beam Search 等。此类解码方式将会输出多个 candidates。有个直观的感受是，在多序列生成过程中，大概率会以一个**树状**的样式增长，因此有可能可以共享某些 KV Cache。然而在传统解码模式下，因为 KV Cache 分开存放在连续的显存空间，显存的占用将会线性增长，无法达成 KV Cache 的共享。 </span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>PagedAttention 以引用计数和 Copy-on-Write 的特性实现了多序列生成的 KV Cache 共享具体，Parallel sampling 的解码流程如下图所示：</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a><span class="al">![annimation3.gif](../attachments/annimation3.gif)</span>{width=70%}</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>在 Beam Search 的场景下，则会管理一个树状结构，在最新时刻的解码结果出现后，可以将废弃的 Block 进行剪枝回收。如下图示例，Beam width 为4，虚线左侧是 $t-1$ 时刻的 KV Cache 占用情况，当 $t$ 时刻解码完成后，最新的 Block9~12 依赖的 KV Cache 全部来自 Block6 和 Block7，则 Block2、4、5、8 的显存将被回收。</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>../attachments</span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a><span class="al">![fc7f0992.png](../attachments/fc7f0992.png)</span>{width=60%}</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>除此之外，在大批量多序列推理的过程中，因为显存的动态分配，在某个时刻有可能会有显存不足的情况，为了防止此类情况，PagedAttention 实现了内存抢占机制，核心思想是 the **earliest** arrived requests are **served first** and the **latest** requests are **preempted first**。具体实现方式有:</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Swapping: latest requests 占用的 Block 将会被放入 CPU RAM，等待前方解码完成释放空间后，再从 CPU RAM 中重新取回显存。</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Recomputation: 直接释放 latest requests 占用的 Block，后续将重新计算得到 KV Cache（在这里重新计算是很快的，因为我们已知当前所有的 token，直接用 Masked Attention并行计算）。</span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5. 抛砖引玉</span></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a>此次分享专注的是 Transformer 在计算和推理上结构优化，该类方法主要通过优化 Transformer 的结构以实现推理性能的提升。</span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>当前 LLM 加速技术是热门的研究方向，目前包括但不限于以下几点(<span class="co">[</span><span class="ot">NLP（十八）：LLM 的推理优化技术纵览</span><span class="co">](https://zhuanlan.zhihu.com/p/642412124)</span>)：</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>子图融合（subgraph fusion）</span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>模型压缩（Model Compression）</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>并行化（Parallelism）</span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transformer 结构优化</span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>动态批处理（Dynamic Batch, Continuous batch）</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>KV cache 优化</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>投机采样与MoE</span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a>**一起学习、讨论和进步。**</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>