[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "",
    "text": "[8 May 2018]Online normalizer calculation for softmax\n[27 May 2022]FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n[17 Jul 2023]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n[12 Sep 2023]Efficient Memory Management for Large Language Model Serving with PagedAttention\nNLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能\nNLP（十八）：LLM 的推理优化技术纵览\nvLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\nFlashAttention 的速度优化原理是怎样的？\n一心二用的Online Softmax\n图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#参考文献",
    "href": "index.html#参考文献",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "",
    "text": "[8 May 2018]Online normalizer calculation for softmax\n[27 May 2022]FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n[17 Jul 2023]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n[12 Sep 2023]Efficient Memory Management for Large Language Model Serving with PagedAttention\nNLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能\nNLP（十八）：LLM 的推理优化技术纵览\nvLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\nFlashAttention 的速度优化原理是怎样的？\n一心二用的Online Softmax\n图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "1. Overview",
    "text": "1. Overview\n\n\n\n\n\n\n\n\n\nTerm\n是什么\n用在哪里\n技术亮点\n\n\n\n\nSoftmax\n-\n-\n-\n\n\nSafeSoftmax\n为了防止 Softmax 计算过程中 exp 函数出现数值上下溢的情况，提出的一种数值处理方案\nTorch,TF,Paddle 等框架，嵌入至绝大多数模型\n数值处理，指数部分减去原始向量的 max 值。\n\n\nOnlineSoftmax\nSoftmax 的一种在线更新的计算方法\n任何需要计算 Softmax 的场景\n降低对原始向量的遍历次数，计算速度加快。\n\n\nFlashAttention\n一种 精确的（非近似） Attention 的加速算法\n所有用到 Attention OP 的模型，包括不限于类 Bert，类 GPT 等模型。\n结合 GPU Memory 的硬件 Layout，基于 Online Softmax 对 Attention OP 进行 精细化 优化。不降 FLOPs，降低 HBM 读写次数和算子融合达到加速效果。时间复杂度 不变，空间复杂度从 \\(O(N^2)\\) 降低至 \\(O(N)\\)。\n\n\nFlashAttention-2\n基于 FlashAttention 算法，提升 FlashAttention 的计算效率\n与 FlashAttention 一致\nFlashAttention 计算提效，提升了非 matmul 的 FLOPs，引入了更好的并行化计算方式和内存复用。\n\n\nPagedAttention\n一种提升模型服务 吞吐量 的算法，提升批量计算时显存的利用效率\nvLLM 加速框架的核心技术，用于各种用到用到 Decoder 模型的服务中，适用于市面上各类 LLM\nLLM 推理框架（如 FT 和 Orca）在批量推理时，由于用于存储 KV Cache 的显存需要提前分配，并针对每一个序列分配在连续的显存空间上，导致存在大量的碎片（存在于 序列内 和 序列间）。PagedAttention 引入内存分页管理的思想，用一个 Scheduler 单例，以 Block 为单位动态地分配显存，提升显存利用率（Batch 计算中复用，Decode 过程中剪枝等），从而加速批量计算，提升模型服务的吞吐量。"
  },
  {
    "objectID": "index.html#onlinesoftmax",
    "href": "index.html#onlinesoftmax",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "2. OnlineSoftmax",
    "text": "2. OnlineSoftmax\n\n2.1 Softmax\n\\[ y = softmax(x) \\] \\[ y_i = \\frac{e^{x_i}}{\\sum_{j=1}^{V} e^{x_j}}  \\tag{1}\\]\nwhere \\(x, y \\in \\mathbb{R}^V\\) . The naive implementation (see algorithm 1) scans the input vector two times - one to calculate the normalization term \\(d_V\\) and another to compute output values \\(y_i\\) - effectively doing three memory accesses per vector element: two loads and one store.\n\n\n\nd20eb1b8.png\n\n\n\n\n2.2 SafeSoftmax\nUnfortunately, on real hardware, where the range of numbers represented is limited, the line 3 of the algorithm 1 can overflow or underflow due to the exponent. There is a safe form of (1), which is immune to this problem:\n\\[ y_i = \\frac{e^{x_i-\\max_{k=1}^{V}x_k}}{\\sum_{j=1}^{V} e^{x_j-\\max_{k=1}^{V}x_k}}  \\tag{2}\\]\n\n\n\nde4cc3d9.png\n\n\nBut Safe Softmax does three passes over input vector: The first one calculates the maximum value \\(m_V\\) , the second one - normalization term \\(d_V\\) , and the third one - final values \\(y_i\\), see algorithm 2; This results in 4 memory access per vector element overall.\n\n\n2.3 OnlineSoftmax\nThe algorithm 3 calculates both the maximum value m and the normalization term \\(d\\) in a single pass over input vector with negligible additional cost of two operations per vector element. It reduces memory accesses from 4 down to 3 per vector element for the Softmax function evaluation. Inspiration came from the numerically stable variance calculation online algorithm.\n\n\n\nba540d94.png\n\n\nEssentially, the algorithm keeps the maximum value \\(m\\) and the normalization term \\(d\\) as it iterates over elements of the input array. At each iteration it needs to adjust the normalizer \\(d\\) to the new maximum \\(m_j\\) and only then add new value to the normalizer.\n这里需要注意的点在于，\\(m_j\\) 和 \\(d_j\\) 是在同一遍 Loop 中实时更新的，\\(m_j\\) 求 max，很容易更新；\\(d_j\\) 是指数项求和，在 Loop 中一旦发生最大值更新，即 \\(m_j \\ne m_{j-1}\\) ，\\(d_{j-1}\\) 则需要进行一个指数项的缩放，其数值为 \\(e^{m_{j-1}-m_j}\\)。\n实际上这里还可以继续优化，把 \\(y\\) 的计算也放在第一遍 Loop 中，如果这么做，每一次 iter 需要更新的值是 \\(\\{y_1, y_2...y_{j-1}\\}\\)，然后拼接上最新的 \\({y_j}\\)，缩放的方式需要同时考虑分子的 \\(m\\) 和分母中的 \\(v\\)，其数值为 \\(\\frac{d_{j-1} * e^{m_{j-1}-m_j}}{d_j}\\)。(注意针对 \\(y\\) 的缩放和针对 \\(d\\) 的缩放的差异)\n\n\n2.4 代码实现\nOnlineSoftmax实现，如 2.3 最后的结论，用单 Loop 完成 SafeSoftmax 的数值计算，且支持 chunk 内并行。\n\n\nCode\nimport numpy as np\n\ndef online_softmax(x: np.ndarray, chunk_size: int = 2) -&gt; np.ndarray:\n    num_steps = int(np.ceil(x.shape[1] / chunk_size))\n    m = np.zeros((x.shape[0]))\n    d = np.zeros((x.shape[0]))\n    softmax = np.zeros_like(x)\n\n    for i in range(num_steps):\n        # Get chunk data\n        chunk_x = x[:, i * chunk_size : (i + 1) * chunk_size]\n\n        # INFO: 1. Perform local softmax on chunk data.\n        chunk_m = np.max(chunk_x, axis=-1)\n        chunk_f = np.exp(chunk_x - np.expand_dims(chunk_m, 1))\n        chunk_d = np.sum(chunk_f, axis=-1)\n        chunk_softmax = chunk_f / np.expand_dims(chunk_d, 1)\n\n        # INFO: 2. Get new m_x and l_x.\n        m_new = np.maximum(m, chunk_m)\n        d_new = d * np.exp(m - m_new) + chunk_d * np.exp(chunk_m - m_new)\n\n        # INFO: 3. Rescale(OLD + NEW) and concat(NEW) softmax values.\n        # Rescale\n        softmax = (\n            softmax\n            * np.expand_dims(d, 1)\n            * np.expand_dims(np.exp(m - m_new), 1)\n            / np.expand_dims(d_new, 1)\n        )\n        # Rescale and concat\n        softmax[:, i * chunk_size : (i + 1) * chunk_size] = (\n            chunk_softmax\n            * np.expand_dims(chunk_d, -1)\n            * np.expand_dims(np.exp(chunk_m - m_new), 1)\n            / np.expand_dims(d_new, -1)\n        )\n\n        # INFO: 4. Update m_x and l_x from CURRENT global info.\n        m = m_new\n        d = d_new\n\n    return softmax"
  },
  {
    "objectID": "index.html#flashattention",
    "href": "index.html#flashattention",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "3. FlashAttention",
    "text": "3. FlashAttention\n有了 OnlineSoftmax 的概念，从 OnlineSoftmax 过渡到 FlashAttention 就很平滑了。\nAttention 的计算核心就是用 \\(Q\\) 和 \\(K\\) 计算 Attention Score，用 Softmax 将 Attention Score 转换到概率空间，然后在 \\(V\\) 做加权获取最终的 Attention 输出。标准的 Dot-Product Self Attention 如下所示：\n\n\nCode\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -&gt; np.ndarray:\n    S = np.matmul(Q, K.T)  # Dot-Product Attention\n    P = safe_softmax(S)  # (N, N)\n    O = np.matmul(P, V)  # (N, d)\n\n    return O\n\n\n从 Attention 端到端的计算上看，Softmax 的数值只是过程变量，且 Softmax 的数值最终呈现在 \\(O\\) 里。在 2.3 小节中，我们知道 Softmax 的数值是可以分块在 Loop 中完成计算的，那么是否有可能在 \\(O\\) 上面做 Online 更新呢？ —— 答案是必然的，有。\n展开 FlashAttention 的计算细节之前我们先整体了解一下当前 GPU 的运算性能限制。\n\n3.1 Compute Bound Vs Memory Bound\n首先了解以下几个概念：\n\n算力 \\(\\pi\\): 也称为计算平台的性能上限，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOPS or FLOP/s。\n带宽 \\(\\beta\\): 也即计算平台的带宽上限，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。\n计算强度上限 \\(I_{max} = \\frac{\\pi}{\\beta}\\): 两个指标相除即可得到计算平台的计算强度上限。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是FLOPs/Byte。\n模型的理论性能 \\(P\\): 我们最关心的指标，即模型在计算平台上实际所能达到的每秒浮点运算次数（理论值）。单位是FLOPS or FLOP/s。\n\n一个算法运行的效率是离不开硬件本身的。我们往往想知道：对于一个运算量为 \\(\\pi_t\\)，数据读取存储量为 \\(\\beta_t\\) 的算法，它在算力上限为 \\(\\pi\\)，带宽上限为 \\(\\beta\\) 的硬件上，能达到的最大性能 \\(P\\) (Attanable Performance)是多少？这里最大性能 \\(P\\) 指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是FLOP/s。\nRoof-line模型就是为了解答这一问题而提出的，它能直观帮我们看到算法在硬件上能跑得多快，模型见下图。\n\n\n\n15426dbd.png\n\n\n如图，横坐标 \\(I\\) 表示计算强度，满足 \\(I=\\frac{\\pi_t}{\\beta_t}\\) ；纵坐标 \\(P\\) 表示算法运行在硬件上的性能。算法的运行性能不会超过硬件本身的计算上限，所以 \\(P\\) 的最大值取到 \\(\\pi\\)。根据我们之前的分析，当 \\(I&gt;\\frac{\\pi}{\\beta}\\) 时，存在计算限制(Compute Bound)；当 \\(I&lt;\\frac{\\pi}{\\beta}\\) 时，存在内存限制(Memory Bound)。\n\n\n3.2 GPU Memory Layout\n\n\n\nace8b864.png\n\n\n在讲计算之前首先了解 GPU 的存储细节。上图最左是 FlashAttention 论文所绘制的硬件不同的存储类型、存储大小和带宽。一般来说，GPU 上的存储分类，可以按照是否在芯片上分为片上内存(on chip)和片下内存(off chip)。\n\n片上内存：主要用于缓存（cache）及少量特殊存储单元（例如texture），其特点是存储空间小，但带宽大。对应到上图中，SRAM 就属于片上内存，它的存储空间只有 20MB(SM 数量*L1 Cache 容量)，但是带宽可以达到 19TB/s。\n片下内存：主要用于全局存储（global memory），即我们常说的显存，其特点是存储空间大，但带宽小，对应到上图中，HBM 就属于片下内存，它的存储空间有 40GB(A100 40GB)，但带宽相比于 SRAM 就小得多，只有 1.5TB/s。\n\n\n\n\n0cc7e3db.png\n\n\n\n\n\n6826c3c0.png\n\n\n从上图的硬件架构上看，越是靠近 SM 核心的存储，读写速度就越快。上图中 L1 Cache 则是 FlashAttention 中的 SRAM，为每个 SM 独占，L2 Cache 是所有 SM 共享，再往下就是 HMB。所以在算法计算中，如果读写全部在 SRAM 上，将达到 IO 速度的上限，计算自然就快了。\n\n\n3.3 Standard Attention\nGiven input sequences \\(Q, K, V \\in \\mathbb{R}^{N \\times d}\\) where \\(N\\) is the sequence length and \\(d\\) is the head dimension, we want to compute the attention output \\(O \\in \\mathbb{R}^{N \\times d}\\):\n\\[ S = QK^T \\in \\mathbb{R}^{N \\times N}, P = softmax(S) \\in \\mathbb{R}^{N \\times N}, O = PV \\in \\mathbb{R}^{N \\times d} \\]\n标准 Attention 的算法如下，所有中间结果储存在 HBM，空间复杂度最高的是 \\(O(N^2)\\) 的 \\(S\\) 和 \\(P\\)，对于 HBM 的 IO(读写) 复杂度是 \\(O(Nd+N^2)\\)\n\n\n\n7e5428b9.png\n\n\nStandard attention implementations materialize the matrices \\(S\\) and \\(P\\) to HBM, which takes \\(O(N^2)\\) memory. Often \\(N\\gg d\\) (e.g., for GPT2, \\(N=1024\\) and \\(d=64\\)(single head dim)). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time.\nThis problem is exacerbated(恶化) by other elementwise operations applied to the attention matrix, such as masking applied to \\(S\\) or dropout applied to \\(P\\).\n\n\n3.4 FlashAttention\nFlashAttention 的核心理念是尽可能少读写 HBM，在具体实现上采用了 tiling（分块）和 recompute（重新计算 \\(S\\) 和 \\(P\\)，因为 forward 过程中不存储 \\(O(N^2)\\) 的 Attention Scores，但是反向需要用到，所以采用了重新计算。）\n\n\n\n718d7308.png\n\n\n同样对 FlashAttention 进行 IO 复杂度分析，\\(M\\) 为单个流式处理器的 L1 Cache（大小约100KB，A100 中有 108 个 SM，每个 SM L1 Cache 是 192KB，总计约 20MB），\\(d\\) 是 single head dim（一般是64/128）。\n首先我们看 Outer Loop，一次 Loop 会访问一遍完整的 \\(K\\) 和 \\(V\\)，IO 复杂度是 \\(O(Nd)\\)。 对于 Inner Loop，一次 Loop 会访问一遍完整的 \\(Q\\)，为 \\(O(Nd)\\)；Inner Loop 的次数 \\(T_c = \\lceil{\\frac{N}{B_c}}\\rceil\\)，为 \\(O(NdM^{-1})\\)，相乘可以得到 Inner Loop 的 IO 复杂度是 \\(O(N^2d^2M^{-1})\\)，因此 FlashAttention 算法整体的 IO 复杂度为 \\(O(Nd + N^2d^2M^{-1})\\)。\n对比标准 Attention 的 \\(O(Nd+N^2)\\)，因为 \\(\\frac{d^2}{M}\\ll1\\)，因此对于 HBM 的 IO 复杂度得到了显著的降低。此外，再加上 kernel fusion，进一步对计算加速。\n同时，因为不需要存储 \\(S\\) 和 \\(P\\)（\\(O(N^2)\\)），\\(m\\) 和 \\(l\\) 的开销小（\\(O(N)\\)），空间复杂度从标准的 \\(O(N^2d)\\) 降低到了 \\(O(Nd)\\)。\n\n\n\nc8105e32.png\n\n\n注意上图左侧中，Linformer Attention 和 OpenAI Sparse Attention 不是 exact Attention。\n\n\n3.5 代码实现\n沿用 Online Softmax 的框架，并从 1d 的 chunk-wise 拓展到 2d 的 block-wise。模型一个 SM 单元中的计算逻辑，此处用 numpy 验证其准确性，无加速效果，只减少了空间复杂度。简单起见忽略了 mask，dropout等细节。\n关于 backward 的计算，感兴趣的可以自行查看论文，核心思想是重新计算 \\(S\\) 和 \\(P\\)，把前向中没有存储的变量进行 recompute，搭回反向的通道。\n\n\nCode\ndef flash_attention(\n    Q: np.ndarray, K: np.ndarray, V: np.ndarray, Br: int = 2, Bc: int = 2\n) -&gt; np.ndarray:\n    N, d = Q.shape\n    O = np.zeros_like(Q)  # Alloc memory for `O` in HBM\n\n    outer_loop_steps = int(np.ceil(N / Bc))\n    inner_loop_steps = int(np.ceil(N / Br))\n    m_x_all = np.zeros((N))\n    l_x_all = np.zeros((N))\n\n    for j in range(outer_loop_steps):\n        for i in range(inner_loop_steps):\n            # Get chunk data\n            chunk_q = Q[i * Br : (i + 1) * Br]  # (Br, d)\n            chunk_k = K[j * Bc : (j + 1) * Bc]  # (Bc, d)\n            chunk_v = V[j * Bc : (j + 1) * Bc]  # (Bc, d)\n            m_x = m_x_all[i * Br : (i + 1) * Br]\n            l_x = l_x_all[i * Br : (i + 1) * Br]\n\n            # INFO: 1. Perform attention on block data.\n            chunk_s = np.matmul(chunk_q, chunk_k.T)  # (Br, Bc)\n            chunk_m_x = np.max(chunk_s, axis=-1)\n            chunk_f_x = np.exp(chunk_s - np.expand_dims(chunk_m_x, 1))\n            chunk_l_x = np.sum(chunk_f_x, axis=-1)\n            chunk_p = chunk_f_x / np.expand_dims(chunk_l_x, 1)\n            chunk_o = np.matmul(chunk_p, chunk_v)\n\n            # INFO: 2. Update `GLOBAL` stats.\n            m_x_new = np.maximum(m_x, chunk_m_x)\n            l_x_new = l_x * np.exp(m_x - m_x_new) + chunk_l_x * np.exp(\n                chunk_m_x - m_x_new\n            )\n\n            # INFO: 3. Rescale and update `GLOBAL` output.\n            # Rescale(Old)\n            O[i * Br : (i + 1) * Br] = (\n                O[i * Br : (i + 1) * Br]\n                * np.expand_dims(l_x, 1)\n                * np.expand_dims(np.exp(m_x - m_x_new), 1)\n                / np.expand_dims(l_x_new, 1)\n            )\n            # Update(New)\n            O[i * Br : (i + 1) * Br] += (\n                chunk_o\n                * np.expand_dims(np.exp(chunk_m_x - m_x_new), 1)\n                / np.expand_dims(l_x_new, -1)\n            )\n\n            m_x_all[i * Br : (i + 1) * Br] = m_x_new\n            l_x_all[i * Br : (i + 1) * Br] = l_x_new\n\n    return O"
  },
  {
    "objectID": "index.html#pagedattention",
    "href": "index.html#pagedattention",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "4. PagedAttention",
    "text": "4. PagedAttention\nPagedAttention 的核心是提升显存利用效率，减少存储碎片，最终达到提升服务吞吐量的效果，不同于 FlashAttention，它对单一的序列生成理论上没有加速效果。\n\n4.1 KV Cache\n\n\n\n24972e88.png\n\n\nFirst, the existing systems suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they pre-allocate a contiguous chunk of memory with the request’s maximum length(e.g., 2048 tokens).\nLLM 模型在做推理时，会对 KV Cache 进行提前的显存分配。因为解码前不知道一个序列最终长度是多少（等待 EOS 或者最长时截断），模型会根据用户传参（max_length）来分配一段最大的连续的显存空间，等待自回归解码时逐个填充（此处用 greedy decoding and sampling 来举例，如果是其他解码方案，显存的分配会更多）。\n\n\n4.2 Memory Fragmentation\n\n\n\nc7d061fa.png\n\n\n上图展示了三种不同的显存浪费的情况：\n\nreserved: 当前解码时刻还未用到的显存(但是后面将会使用，假设我们已经知道输出序列。)\ninternal fragmentation: 上面提到的根据 max_length 提前分配的显存，在 batch 推理的过程中，这部分将持续占用，但不产生任何价值。\nexternal fragmentation: 推理框架（文中举例是 FT 和 Orca），在为多个序列分配各自的连续的显存空间存在的碎片区域。\n\n\n\n4.3 PagedAttention\nIn vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is:\n\nLarge: Takes up to 1.7GB for a single sequence in LLaMA-13B.\nDynamic: Its size depends on the sequence length, which is highly variable and unpredictable. As a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste 60% – 80% of memory due to fragmentation and over-reservation.\n\nTo address this problem, we introduce PagedAttention, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.\n\n4.3.1 Memory Block\n\n\n\nannimation0.gif\n\n\n上图演示的是最新一个 token for 对于已有的序列（可以是输入的 prompt）Alan Turing is a computer scientist and mathematician renowned for 做 Attention 的过程。图示中以 Block 分块管理显存，以达到用不连续代替连续的显存空间。\nPagedAttention 想要达到最小化的显存浪费，在上文中介绍的 reserved 和 external fragmentation在这种方式下可以完全清除，但 internal fragmentation 无法完全消除。图中 Block 的 size 为 4，在 Block 2 中存在 2 个单元的碎片浪费。\n\n\n4.3.2 Block table\nBecause the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous logical blocks of a sequence are mapped to non-contiguous physical blocks via a block table. The physical blocks are allocated on demand as new tokens are generated.\n\n\n\nannimation1.gif\n\n\nPagedAttention 在实现上采用了 KV Cache Manager 维护了多张 Block table，表里存着 logical blocks 到 phsical blocks 的映射。并且，为了尽量消除 internal fragmentation，他实现了动态的显存分配管理。\n\n\n4.3.3 Memory Sharing\n\n\n\nannimation2.gif\n\n\n在 LLM 解码过程中，我们可能会采取多种解码方式扩大搜索空间，如 Parallel sampling、Beam Search 等。此类解码方式将会输出多个 candidates。有个直观的感受是，在多序列生成过程中，大概率会以一个树状的样式增长，因此有可能可以共享某些 KV Cache。然而在传统解码模式下，因为 KV Cache 分开存放在连续的显存空间，显存的占用将会线性增长，无法达成 KV Cache 的共享。\nPagedAttention 以引用计数和 Copy-on-Write 的特性实现了多序列生成的 KV Cache 共享具体，Parallel sampling 的解码流程如下图所示：\n\n\n\nannimation3.gif\n\n\n在 Beam Search 的场景下，则会管理一个树状结构，在最新时刻的解码结果出现后，可以将废弃的 Block 进行剪枝回收。如下图示例，Beam width 为4，虚线左侧是 \\(t-1\\) 时刻的 KV Cache 占用情况，当 \\(t\\) 时刻解码完成后，最新的 Block9~12 依赖的 KV Cache 全部来自 Block6 和 Block7，则 Block2、4、5、8 的显存将被回收。\n\n\n\nfc7f0992.png\n\n\n除此之外，在大批量多序列推理的过程中，因为显存的动态分配，在某个时刻有可能会有显存不足的情况，为了防止此类情况，PagedAttention 实现了内存抢占机制，核心思想是 the earliest arrived requests are served first and the latest requests are preempted first。具体实现方式有:\n\nSwapping: latest requests 占用的 Block 将会被放入 CPU RAM，等待前方解码完成释放空间后，再从 CPU RAM 中重新取回显存。\nRecomputation: 直接释放 latest requests 占用的 Block，后续将重新计算得到 KV Cache（在这里重新计算是很快的，因为我们已知当前所有的 token，直接用 Masked Attention并行计算）。"
  },
  {
    "objectID": "index.html#抛砖引玉",
    "href": "index.html#抛砖引玉",
    "title": "LLM 加速技术：FlashAttention 和 PagedAttention",
    "section": "5. 抛砖引玉",
    "text": "5. 抛砖引玉\n此次分享专注的是 Transformer 在计算和推理上结构优化，该类方法主要通过优化 Transformer 的结构以实现推理性能的提升。\n当前 LLM 加速技术是热门的研究方向，目前包括但不限于以下几点(NLP（十八）：LLM 的推理优化技术纵览)：\n\n子图融合（subgraph fusion）\n模型压缩（Model Compression）\n并行化（Parallelism）\nTransformer 结构优化\n动态批处理（Dynamic Batch, Continuous batch）\nKV cache 优化\n投机采样与MoE\n\n一起学习、讨论和进步。"
  }
]